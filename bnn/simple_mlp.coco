import numpy as np
import tensorflow as tf

from ml import wrangle
from ml.datasets.mnist import load_mnist

if __name__ == '__main__':
    # Placeholders
    features = tf.placeholder('float', [None, 784])
    labels = tf.placeholder('float', [None, 10])

    # Computation graph
    weights = {
            'hidden': tf.Variable(tf.random_normal([784, 100])),
            'output': tf.Variable(tf.random_normal([100, 10]))
            }
    biases =  {
            'hidden': tf.Variable(tf.random_normal([100])),
            'output': tf.Variable(tf.random_normal([10]))
            }
    cross_entropy = (
        features
            |> xs -> (xs `tf.matmul` weights['hidden']) + biases['hidden']
            |> hs -> (hs `tf.matmul` weights['output']) + biases['output']
            |> ls -> tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=ls)
            |> tf.reduce_mean
        )
    train_op = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)

    # Training
    mnist = load_mnist()
    split = wrangle.minibatch_splitter(512)
    with tf.Session() as session:
        def train_one_step(xs, ys):
            _, cost = session.run([train_op, cross_entropy], feed_dict={features: xs, labels: ys})
            return cost
        def train_one_epoch():
            return (
                split(mnist['train']['features'], mnist['train']['labels'])
                    |> map$(pair -> train_one_step(*pair))
                    |> list
                )
        session.run(tf.global_variables_initializer())
        range(0, 10) |> map$(_ -> train_one_epoch()) |> map$(np.mean) |> list |> print
